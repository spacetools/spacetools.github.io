<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</title>
    
    <!-- Primary Meta Tags -->
    <meta name="title" content="SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL">
    <meta name="description" content="SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL), enabled by our Toolshed infrastructure. Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation.">
    <meta name="keywords" content="vision language models, spatial reasoning, reinforcement learning, robotics, computer vision, tool-augmented AI">
    <meta name="author" content="Siyi Chen, Mikaela Angelina Uy, Chan Hee Song, Faisal Ladhak, Adithyavairavan Murali, Qing Qu, Stan Birchfield, Valts Blukis, Jonathan Tremblay">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://spacetools.github.io/">
    <meta property="og:title" content="SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL">
    <meta property="og:description" content="SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL). Achieves state-of-the-art performance on spatial reasoning benchmarks and enables precise real-world robot manipulation.">
    <meta property="og:image" content="https://spacetools.github.io/assets/teaser.jpg">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:site_name" content="SpaceTools">
    
    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://spacetools.github.io/">
    <meta name="twitter:title" content="SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL">
    <meta name="twitter:description" content="SpaceTools empowers VLMs with vision and robotic tools for spatial reasoning via Double Interactive Reinforcement Learning (DIRL). Achieves state-of-the-art performance on spatial reasoning benchmarks.">
    <meta name="twitter:image" content="https://spacetools.github.io/assets/teaser.jpg">
    
    <!-- Additional Meta Tags -->
    <meta name="robots" content="index, follow">
    <link rel="canonical" href="https://spacetools.github.io/">
    
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Roboto+Slab:wght@300;400;700&display=swap"
        rel="stylesheet">
</head>

<body>
    <header>
        <div class="container">
            <h1>SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</h1>
            <div class="authors">
                <span class="author">Siyi Chen<sup>1,2,*</sup></span>
                <span class="author">Mikaela Angelina Uy<sup>2</sup></span>
                <span class="author">Chan Hee Song<sup>3</sup></span>
                <span class="author">Faisal Ladhak<sup>2</sup></span>
                <span class="author">Adithyavairavan Murali<sup>2</sup></span>
                <span class="author">Qing Qu<sup>1</sup></span>
                <span class="author">Stan Birchfield<sup>2</sup></span>
                <span class="author">Valts Blukis<sup>&Dagger;,2</sup></span>
                <span class="author">Jonathan Tremblay<sup>&Dagger;,2</sup></span>
            </div>
            <div class="affiliations">
                <span class="affiliation"><sup>1</sup>University of Michigan</span>
                <span class="affiliation"><sup>2</sup>NVIDIA</span>
                <span class="affiliation"><sup>3</sup>Ohio State University</span>
                <span class="note"><sup>&Dagger;</sup>Project Leads</span>
                <span class="note"><sup>*</sup>Work done during an internship at NVIDIA</span>
            </div>
            <div class="links">
                <a href="https://arxiv.org/pdf/2512.04069"
                    class="btn">Paper</a>
                <a href="https://arxiv.org/abs/2512.04069"
                    class="btn">arXiv</a>
                <a href="https://github.com/spacetools/SpaceTools"
                class="btn">Code (Coming Soon)</a>
                <!-- <span class="btn disabled">Code (Coming Soon)</span> -->
                <span class="btn disabled">Data (Coming Soon)</span>
                <span class="btn disabled">Model (Coming Soon)</span>
            </div>
            <div class="tldr">
                <p><strong>TL;DR:</strong> <strong>SpaceTools</strong> empowers VLMs with 
                    vision and robotic tools for spatial reasoning 
                    via <strong>Double Interactive Reinforcement Learning (DIRL)</strong>, 
                    enabled by our <strong>Toolshed</strong> infrastructure. 
                    Achieves state-of-the-art performance on spatial reasoning benchmarks and
                    enables precise real-world robot manipulation.</p>
            </div>
            <div class="demo-video">
                <video controls class="demo-video-player">
                    <source src="assets/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>
    </header>

    <!-- <section class="hero">
        <div class="container">
            <img src="assets/teaser.jpg" alt="SpaceTools Teaser" class="teaser-img">
            <p class="caption"><strong>SpaceTools</strong> uses multiple computer vision tools to solve complex
                problems.</p>
        </div>
    </section> -->

    <section class="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p>
                Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with
                metrically precise spatial reasoning required for embodied applications.
                The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these
                capabilities, such as depth estimators, segmentation models, and pose estimators.
                Yet it remains an open challenge how to realize this vision without solely relying on handcrafted
                prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover
                optimal tool-use patterns.
                Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single
                visual tool due to the large search space in multi-tool reasoning.
            </p>
            <p>
                We introduce <strong>Double Interactive Reinforcement Learning (DIRL)</strong>, a two-phase training
                framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In
                the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL
                with traces from a frontier model using all tools.
                In the exploration phase, the model further refines multi-tool coordination through continued RL.
                Our model, <strong>SpaceTools</strong>, with tool-augmented spatial reasoning ability, achieves
                state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ask) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL
                provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on
                RoboSpatial) baselines.
            </p>
        </div>
        <div class="container">
            <img src="assets/teaser.jpg" alt="SpaceTools Teaser" class="teaser-img">
            <p class="caption"><strong>SpaceTools</strong> learns to use multiple computer vision tools to solve complex
                problems.</p>
        </div>
    </section>

    <!-- <section class="demo">
        <div class="container">
            <h2>Demo</h2>
            <img src="assets/demo.png" alt="SpaceTools Demo" class="demo-img">
        </div>
    </section> -->

    <section class="reasoning_visualization">
        <div class="container">
            <h2>Spatial Reasoning Visualization</h2>
            <img src="assets/traces.png" alt="SpaceTools Reasoning Visualization" class="reasoning-visualization-img">
            <p class="caption">Spatial reasoning visualization of <strong>SpaceTools</strong>. It performs diverse spatial reasoning tasks including relative depth, pose, grasp, spatial compatibility, and spatial relationship by interleaving reasoning (gray) and vision tool calls (green) before producing the final answer.</p>
        </div>
    </section>

    <section class="results">
        <div class="container">
            <h2>Quantitative Results</h2>
            <div class="table-responsive">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th rowspan="2">Model</th>
                            <th colspan="3"><a href="https://chanh.ee/RoboSpatial/">RoboSpatial</a></th>
                            <th><a href="https://zeyofu.github.io/blink/">BLINK</a></th>
                            <th rowspan="2"><a
                                    href="https://huggingface.co/datasets/JingkunAn/RefSpatial">RefSpatial</a></th>
                            <th colspan="2"><a href="https://huggingface.co/datasets/nyu-visionx/CV-Bench">CVBench</a>
                            </th>
                            <th colspan="3"><a href="https://bop-ask.github.io/">BOP-ask</a></th>
                        </tr>
                        <tr>
                            <th>VQA</th>
                            <th>Vacant</th>
                            <th>Overall</th>
                            <th>Depth</th>
                            <th>2D Rel.</th>
                            <th>3D Depth</th>
                            <th>Pose</th>
                            <th>Grasp-MACE</th>
                            <th>Grasp-SR</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="group-header">
                            <td colspan="11">Proprietary Models</td>
                        </tr>
                        <tr>
                            <td>Claude Sonnet 4.5</td>
                            <td>75.44</td>
                            <td>23.77</td>
                            <td>57.43</td>
                            <td>78.23</td>
                            <td>7.49</td>
                            <td>89.85</td>
                            <td>78.50</td>
                            <td>1.67</td>
                            <td>40.12</td>
                            <td class="underline">48.33</td>
                        </tr>
                        <tr>
                            <td>GPT-4o</td>
                            <td>61.61</td>
                            <td>25.10</td>
                            <td>48.88</td>
                            <td>63.71</td>
                            <td>8.48</td>
                            <td>88.77</td>
                            <td>75.50</td>
                            <td>0.00</td>
                            <td>5.50</td>
                            <td>1.67</td>
                        </tr>
                        <tr>
                            <td>GPT-5</td>
                            <td>76.50</td>
                            <td>22.17</td>
                            <td>58.39</td>
                            <td>66.13</td>
                            <td>23.10</td>
                            <td class="underline">95.54</td>
                            <td>91.33</td>
                            <td>9.03</td>
                            <td class="underline">39.59</td>
                            <td>41.67</td>
                        </tr>
                        <tr>
                            <td>Gemini-ER 1.5</td>
                            <td class="underline">79.30</td>
                            <td>31.10</td>
                            <td class="underline">62.50</td>
                            <td>69.23</td>
                            <td>41.72</td>
                            <td class="underline">95.54</td>
                            <td>90.50</td>
                            <td>0.00</td>
                            <td>30.06</td>
                            <td>23.33</td>
                        </tr>

                        <tr class="group-header">
                            <td colspan="11">General Open-Source Models</td>
                        </tr>
                        <tr>
                            <td>LLaVA-NeXT-8B</td>
                            <td>69.31</td>
                            <td>0.00</td>
                            <td>45.15</td>
                            <td>53.23</td>
                            <td>0.78</td>
                            <td>72.15</td>
                            <td>73.67</td>
                            <td>0.00</td>
                            <td>5.04</td>
                            <td>1.67</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-VL-32B</td>
                            <td>61.84</td>
                            <td>3.28</td>
                            <td>41.43</td>
                            <td>70.16</td>
                            <td>7.28</td>
                            <td>90.46</td>
                            <td>86.67</td>
                            <td>0.00</td>
                            <td>29.86</td>
                            <td>23.33</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-VL-3B</td>
                            <td>53.07</td>
                            <td>0.00</td>
                            <td>35.71</td>
                            <td>70.98</td>
                            <td>0.00</td>
                            <td>70.62</td>
                            <td>65.33</td>
                            <td>0.00</td>
                            <td>6.06</td>
                            <td>0.00</td>
                        </tr>

                        <tr class="group-header">
                            <td colspan="11">Spatial VLMs</td>
                        </tr>
                        <tr>
                            <td>SpaceLLaVA-13B</td>
                            <td>61.00</td>
                            <td>2.50</td>
                            <td>40.61</td>
                            <td>51.61</td>
                            <td>3.25</td>
                            <td>61.08</td>
                            <td>62.83</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                        </tr>
                        <tr>
                            <td>RoboPoint-13B</td>
                            <td>70.18</td>
                            <td>19.70</td>
                            <td>52.58</td>
                            <td>54.84</td>
                            <td>15.59</td>
                            <td>74.00</td>
                            <td>76.50</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                        </tr>
                        <tr>
                            <td>Molmo-7B</td>
                            <td>39.92</td>
                            <td>0.82</td>
                            <td>26.29</td>
                            <td>54.03</td>
                            <td>0.00</td>
                            <td>72.15</td>
                            <td>73.33</td>
                            <td>0.00</td>
                            <td>36.74</td>
                            <td>18.33</td>
                        </tr>
                        <tr>
                            <td>RoboBrain2.0-7B</td>
                            <td>59.64</td>
                            <td>44.35</td>
                            <td>54.31</td>
                            <td>84.68</td>
                            <td>32.50</td>
                            <td>87.23</td>
                            <td>90.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                        </tr>
                        <tr>
                            <td>RoboRefer-8B-SFT</td>
                            <td>58.33</td>
                            <td class="bold">61.48</td>
                            <td>59.43</td>
                            <td class="underline">88.71</td>
                            <td class="underline">48.37</td>
                            <td class="bold">96.31</td>
                            <td class="bold">96.50</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                        </tr>

                        <tr class="group-header">
                            <td colspan="11">Tool-free Fine-tuning</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-VL-3B-Tool-free SFT</td>
                            <td>66.66</td>
                            <td>41.80</td>
                            <td>58.00</td>
                            <td>80.65</td>
                            <td>20.22</td>
                            <td>91.54</td>
                            <td>83.33</td>
                            <td>2.44</td>
                            <td>39.47</td>
                            <td>35.00</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5-VL-3B-Tool-free RL</td>
                            <td>67.54</td>
                            <td>28.69</td>
                            <td>54.00</td>
                            <td>80.65</td>
                            <td>23.10</td>
                            <td>87.38</td>
                            <td>70.83</td>
                            <td class="underline">12.00</td>
                            <td>38.79</td>
                            <td>36.67</td>
                        </tr>

                        <tr class="highlight-row">
                            <td class="bold">SpaceTools-3B (Ours)</td>
                            <td class="bold">79.38</td>
                            <td class="underline">52.46</td>
                            <td class="bold">70.00</td>
                            <td class="bold">90.32</td>
                            <td class="bold">53.07</td>
                            <td>94.92</td>
                            <td class="underline">96.00</td>
                            <td class="bold">34.37</td>
                            <td class="bold">43.06</td>
                            <td class="bold">50.00</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <p class="caption">Performance comparison across spatial reasoning benchmarks. All values are normalized
                accuracy (%). <strong>Bold</strong> indicates the best performance within each column, and
                <u>underline</u> denotes the second-best result.
            </p>
        </div>
    </section>

    <section class="robot-execution">
        <div class="container">
            <h2>Real-World Robot Execution</h2>
            <div class="robot-images">
                <div class="robot-demo">
                    <img src="assets/robot.png" alt="SpaceTools Robot Execution" class="robot-img">
                    <p class="robot-caption">Demo 1</p>
                </div>
                <div class="robot-demo">
                    <img src="assets/robot2.png" alt="SpaceTools Robot Execution" class="robot-img">
                    <p class="robot-caption">Demo 2</p>
                </div>
                <div class="robot-demo">
                    <img src="assets/robot3.png" alt="SpaceTools Robot Execution" class="robot-img">
                    <p class="robot-caption">Demo 3</p>
                </div>
            </div>
            <p class="caption"><strong>SpaceTools</strong> demonstrates reliable real-world manipulation using a 7-DOF
                robot as a tool. The model completes multi-step tasks via alternating reasoning (gray), vision tools (green) for perception, and robot tools (blue) for action.</p>
        </div>
    </section>

    <section class="approach">
        <div class="container">
            <h2>Approach</h2>
            
            <div class="approach-subsection">
                <h3>The Key Steps of DIRL</h3>
                <p>
                    <strong>Double Interactive Reinforcement Learning (DIRL)</strong> is a two-phase training framework 
                    designed to teach VLMs reliable and scalable multi-tool coordination. It progresses through:
                </p>
                <ul class="approach-steps">
                    <li>
                        <strong>Teaching Phase:</strong> We construct a curated training set by combining demonstrations 
                        from (a) a single-tool IRL-trained specialist with (b) high-quality multi-tool traces generated by a frontier 
                        model. This phase gives the model strong initialization and clear examples of grounded tool usage.
                    </li>
                    <li>
                        <strong>Exploration Phase:</strong> Starting from this initialization, the model undergoes 
                        full multi-tool interactive RL. Through exploration and feedback, it learns to sequence tools 
                        effectively and refine its coordination strategies.
                    </li>
                </ul>
            </div>

            <div class="approach-highlight-box">
                <p>The interactive RL, the collection of real tool-use traces with frontier models, and the inference-time tool deployment, are made possible 
                    by our scalable <strong>Toolshed</strong> infrastructure.</p>
            </div>

            <div class="approach-subsection">
                <h3>Toolshed and Interactive RL Training Visualization</h3>
                <div class="toolshed-images">
                    <img src="assets/tools.png" alt="Toolshed Infrastructure" class="approach-img">
                    <img src="assets/rl_training_onecol.png" alt="Interactive RL Training" class="approach-img">
                </div>
                <p class="caption"><strong>Left:</strong> Toolshed is an infrastructure for deploying heavy tools during training 
                    and inference. It enables efficient tool execution and management for the DIRL framework.<br>
                    <strong>Right:</strong> Visualization of the interactive reinforcement learning training process (via GRPO), showing how 
                    the model learns to coordinate multiple tools through exploration and feedback.</p>
            </div>

            <div class="approach-subsection">
                <h3>Core Features of Toolshed</h3>
                <p>
                    <strong>Toolshed</strong> is a scalable, distributed, asynchronous framework for deploying compute-heavy vision and robotic tools alongside VLM training and inference. 
                    It mitigates bottlenecks through resource and environment isolation, decoupled tool execution, and asynchronous parallel workers that scale 
                    independently from model compute. Toolshed hosts modular vision tools (<em>e.g.</em>, segmentation, pointing, depth, 3D box fitting, grasp prediction) 
                    and robotic tools (<em>e.g.</em>, image capture, grasp execution, placement). It has the following core features:
                </p>
                <!-- <p>
                    Tool-augmented spatial reasoning requires multi-turn communication with heavy CV modules that often dominate runtime. 
                    Na√Øvely, a single blocking tool call can stall an entire batched rollout. Toolshed avoids this by keeping tools continuously 
                    available and serving requests in parallel as a distributed, asynchronous toolkit.
                </p> -->
            
                <ul class="approach-steps">
                    <li><strong>Decoupled execution.</strong> Tool calls run outside the policy loop, preventing blocking.</li>
                    <li><strong>Asynchronous workers.</strong> Parallel tool instances handle requests independently for high throughput.</li>
                    <li><strong>Resource isolation.</strong> Each tool receives dedicated GPU/CPU resources.</li>
                    <li><strong>Environment isolation.</strong> Tools run in separate Python environments to avoid dependency conflicts.</li>
                    <li><strong>Elastic scaling.</strong> Additional workers can be spawned to handle usage spikes.</li>
                    <li><strong>Multimodal data passing.</strong> Efficient transfer of text, images, and structured outputs (<em>e.g.</em>, point clouds) across devices/nodes.</li>
                </ul>
            </div>
            
        </div>
    </section>

    <section class="citation">
        <div class="container">
            <h2>Citation</h2>
            <div class="citation-block">
                <pre><code><span class="cite-type">@misc</span><span class="cite-brace">{</span><span class="cite-key">chen2025spacetoolstoolaugmentedspatialreasoning</span><span class="cite-brace">,</span>
    <span class="cite-field-wrap"><span class="cite-field">title</span><span class="cite-operator">=</span></span><span class="cite-brace">{</span><span class="cite-title">SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</span><span class="cite-brace">}</span><span class="cite-punct">,</span> 
    <span class="cite-field-wrap"><span class="cite-field">author</span><span class="cite-operator">=</span></span><span class="cite-brace">{</span><span class="cite-author">Siyi Chen and Mikaela Angelina Uy and Chan Hee Song and Faisal Ladhak and Adithyavairavan Murali and Qing Qu and Stan Birchfield and Valts Blukis and Jonathan Tremblay</span><span class="cite-brace">}</span><span class="cite-punct">,</span>
    <span class="cite-field-wrap"><span class="cite-field">year</span><span class="cite-operator">=</span></span><span class="cite-brace">{</span><span class="cite-value">2025</span><span class="cite-brace">}</span><span class="cite-punct">,</span>
    <span class="cite-field-wrap"><span class="cite-field">eprint</span><span class="cite-operator">=</span></span><span class="cite-brace">{</span><span class="cite-value">2512.04069</span><span class="cite-brace">}</span><span class="cite-punct">,</span>
    <span class="cite-field-wrap"><span class="cite-field">archivePrefix</span><span class="cite-operator">=</span></span><span class="cite-brace">{</span><span class="cite-value">arXiv</span><span class="cite-brace">}</span><span class="cite-punct">,</span>
    <span class="cite-field-wrap"><span class="cite-field">primaryClass</span><span class="cite-operator">=</span></span><span class="cite-brace">{</span><span class="cite-value">cs.CV</span><span class="cite-brace">}</span><span class="cite-punct">,</span>
    <span class="cite-field-wrap"><span class="cite-field">url</span><span class="cite-operator">=</span></span><span class="cite-brace">{</span><span class="cite-url">https://arxiv.org/abs/2512.04069</span><span class="cite-brace">}</span>
<span class="cite-brace">}</span></code></pre>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2026 SpaceTools Project</p>
        </div>
    </footer>
</body>

</html>